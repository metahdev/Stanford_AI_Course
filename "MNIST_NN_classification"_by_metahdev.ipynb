{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "\"MNIST_NN_classification\" by metahdev.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTJsCDqM7Xc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural network (you do not have to do anything in this cell)\n",
        "\n",
        "# import the libraries that we will need\n",
        "from sklearn.datasets import fetch_openml  # !!! make sure you check out                                           \n",
        "import numpy as np                         # openml.org for more datasets!\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Now we load the MNIST dataset using openml\n",
        "mnist = fetch_openml('mnist_784', cache=False)  # !!! make sure you check out\n",
        "                                                # openml.org for more datasets!\n",
        "X = mnist.data\n",
        "y = mnist.target\n",
        "\n",
        "# let's normalize X so that all values are between zero and one\n",
        "X = X/np.max(X)\n",
        "\n",
        "# let's convert y into a sparse matrix\n",
        "Ndatapoints = y.shape[0]\n",
        "Nlabels = len(np.unique(y))\n",
        "y_sparse = np.zeros([Ndatapoints, Nlabels])\n",
        "for i in range(Ndatapoints):\n",
        "  y_sparse[i,int(y[i])] = 1\n",
        "y = y_sparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZExLCWA7iCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "5ac942b1-333d-4c21-8d40-bf71dda99a08"
      },
      "source": [
        "# now we split the data (you do not have to do anything in this cell)\n",
        "\n",
        "# because we want to use cross validation, \n",
        "# we randomly select 10% as test, 10% as validation, and 80% as training\n",
        "Ntotal = X.shape[0]\n",
        "Ntest_val = int(Ntotal/5)\n",
        "Nval = Ntest_val/2\n",
        "Ntrain = Ntotal - Ntest_val\n",
        "\n",
        "# now let's generate the indices for the test and val\n",
        "test_val_idx = np.random.choice(range(Ntotal),Ntest_val,replace=False)\n",
        "test_idx = test_val_idx[:int(Ntest_val/2)]\n",
        "val_idx = test_val_idx[int(Ntest_val/2):]\n",
        "\n",
        "X_ts = X[test_idx]\n",
        "y_ts = y[test_idx]\n",
        "X_vl = X[val_idx]\n",
        "y_vl = y[val_idx]\n",
        "X_tr = np.delete(X, test_val_idx, axis=0)\n",
        "y_tr = np.delete(y, test_val_idx, axis=0)\n",
        "\n",
        "# we forget about the testing data for now\n",
        "\n",
        "# let's visualize six examples in the training set\n",
        "Nexamples = 6\n",
        "example_idxs = np.random.choice(Ntrain,Nexamples,replace=False)\n",
        "for i, iexample in enumerate(example_idxs):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(np.reshape(X_tr[iexample],[28,28]),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE4a8pUfsMcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some variables that we will need\n",
        "Ntrain = X_tr.shape[0]\n",
        "Nfeatures = X_tr.shape[1]\n",
        "Nclasses = y_sparse.shape[1]\n",
        "Nval = X_vl.shape[0]\n",
        "\n",
        "# the number of \"neurons\" in the hidden layer\n",
        "Nhidden1 = 16\n",
        "\n",
        "# initialize the parameters that we will train\n",
        "W_1 = np.random.uniform(0.0, 1.0, size = (Nfeatures, Nclasses))\n",
        "b_1 = np.zeros(Nclasses)\n",
        "W_o = np.random.uniform(0.0, 1.0, size = (Nfeatures, Nclasses))\n",
        "b_o = np.zeros(Nclasses)\n",
        "\n",
        "def f1(h1_raw): \n",
        "  result = h1_raw\n",
        "  return result\n",
        "\n",
        "def softmax (tetta):\n",
        "  result = tetta\n",
        " \n",
        "  for iscore in range(tetta.shape[0]):\n",
        "    isum = sum(np.exp(tetta[iscore,:]))\n",
        " \n",
        "    for i in range(tetta.shape[1]):\n",
        "      result[iscore, i] = np.exp(tetta[iscore, i]) / isum\n",
        "  return result\n",
        "\n",
        "# now we can get y_hat for the training data\n",
        "scores = np.dot(X_tr, W_1) + b_1\n",
        "h1_tr = f1(scores) * W_o\n",
        "y_hat = softmax(h1_tr + b_o)\n",
        "\n",
        "# define the learning rate\n",
        "learning_rate = 0.001\n",
        "reg_str = pow(W, 2)\n",
        "\n",
        "# now we put everything in a for loop, so that we can repeat the process\n",
        "Niters = 1000\n",
        "\n",
        "for iter in range(Niters):\n",
        "  ###################################\n",
        "  ### Neural Network Forward Pass ###\n",
        "  ###################################\n",
        "  # calculate the y_hat with the slope and bias values\n",
        "  # for the training set\n",
        "  scores = np.dot(X_tr, W_1) + b_1\n",
        "  h1_tr = f1(scores) * W_o\n",
        "  y_hat = softmax(h1_tr + b_o)\n",
        "  # and also the cost function  \n",
        "  J = (1 / Ntrain) * sum((y_tr) * np.log(y_hat)) \n",
        "  \n",
        "  # now do the same for the validation set\n",
        "  scores = np.dot(X_vl, W_1) + b_1\n",
        "  h1_vl = f1(scores) * W_o\n",
        "  y_hat_vl = softmax(h1_tr + b_o)\n",
        "  # and also the cost function\n",
        "  J_vl = (1 / Nval) * sum((y_vl) * np.log(y_hat)) \n",
        "  \n",
        "  ####################################\n",
        "  ### Neural Network Backward Pass ###\n",
        "  #################################### \n",
        "  # update the bias and weights\n",
        "  dJdscores = y_hat - y_tr\n",
        "  dJdh1 = \n",
        "  dJdh1raw = \n",
        "  dJdb_o = np.dot(np.ones((1, Ntrain)), (dJdscores))\n",
        "  dJdW_o = (1 / Ntrain) * np.dot(np.transpose(X_tr), (dJdscores))\n",
        "  dJdb_1 = np.dot(np.ones((1, Ntrain)), (dJdscores))\n",
        "  dJdW_1 = (1 / Ntrain) * np.dot(np.transpose(X_tr), (dJdscores))\n",
        "  b_o = b_o - learning_rate*dJdb_o\n",
        "  W_o = W_o - learning_rate*dJdW_o\n",
        "  b_1 = b_1 - learning_rate*dJdb_1\n",
        "  W_1 = W_1 - learning_rate*dJdW_1\n",
        "\n",
        "  print('At iteration No. ' + str(iter) + ' ,the cross-entropy (training) cost is: ', J)\n",
        "  print('------------------------------ the cross-entropy (validation) cost is: ', J_vl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Kjf8CrM4zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now let's visualize some of the weights in the first layer\n",
        "for i in range(Nclasses):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  plt.imshow(np.reshape(W_1[:,i],[28,28]),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIiyErEaCy7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we calculate the accuracy\n",
        "\n",
        "y_hat_index = np.argmax(y_hat,axis=1)\n",
        "y_hat_vl_index = np.argmax(y_hat_vl,axis=1)\n",
        "y_tr_index = np.argmax(y_tr,axis=1)\n",
        "y_vl_index = np.argmax(y_vl,axis=1)\n",
        "\n",
        "print('The training accuracy is: ', np.sum(y_tr_index==y_hat_index)/Ntrain)\n",
        "print('The validation accuracy is: ', np.sum(y_vl_index==y_hat_vl_index)/Nval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUtW6wQ-cAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# more things to try:\n",
        "\n",
        "# try different activation functions in different layers.\n",
        "\n",
        "# implement L1 regularization\n",
        "\n",
        "# add more layers to the neural network. \n",
        "# More layers usually results in better accuracy\n",
        "\n",
        "# However, the more layers you add, the slower the neural network\n",
        "# will train. How can you speed up your network?\n",
        "\n",
        "# Get the best validation accuracy by trying out different learning rate\n",
        "# regularization strength, initial conditions for bias and Weight terms, \n",
        "# changing the statistics of the input\n",
        "\n",
        "# can you bring the training accuracy to 100%? If you do everything right\n",
        "# and you do not worry about the validation accuracy, you should be able to\n",
        "# bring the training accuracy to 100% and the training cost very close to 0\n",
        "# hint: you will have to change the size of the training set to do this\n",
        "\n",
        "# what is happening when you bring the training accuracy to 100%? \n",
        "# What happened to the validation accuracy? Can you bring both the training \n",
        "# and validation accuracy to 100%?\n",
        "\n",
        "# how do you know when to stop your training? define a rule to stop your\n",
        "# training, and substitute the for loop for a while loop\n",
        "\n",
        "# once you find the absolue best model, check the test accuracy. \n",
        "# do this only once. Doing it more than one time is bad practice and makes\n",
        "# your cross-validation efforts useless."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}